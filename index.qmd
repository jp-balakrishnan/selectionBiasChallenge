---
title: "Selection Bias & Missing Data Challenge"
subtitle: "Creating a Statistics Meme: Write Your Own Functions"
format:
  html: default
execute:
  echo: false
  eval: true
---

# ðŸŽ¨ Selection Bias & Missing Data Challenge

::: {.callout-important}
## ðŸ“Š Challenge Overview

** Task:** Create a four-panel statistics meme demonstrating selection bias. You'll write three Python functions yourself to complete the workflow, then assemble them into a professional meme.

```{python}
#| label: final-meme
#| echo: false
#| eval: true
#| fig-cap: "Statistics meme demonstrating selection bias"

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from step1_prepare_image import prepare_image

# Load and prepare the image
img_path = 'ai_father.jpg'  # Example image - replace with your own image
gray_image = prepare_image(img_path, max_size=512)

from step2_create_stipple import create_stipple

# Create stippled image
stipple_pattern, samples = create_stipple(
    gray_image,
    percentage=0.08,  # 8% of pixels will be stippled
    sigma=0.9,  # Repulsion radius
    content_bias=0.9,  # Strongly follow importance map
    noise_scale_factor=0.1,  # Moderate exploration
    extreme_downweight=0.5,  # Moderate downweighting of extremes
    extreme_threshold_low=0.2,  # Downweight tones below 0.2
    extreme_threshold_high=0.8,  # Downweight tones above 0.8
    extreme_sigma=0.1  # Smooth transition width
)

from step4_create_block_letter import create_block_letter_g
 
# Get image dimensions
h, w = gray_image.shape
 
# Create block letter A
block_letter = create_block_letter_g(h, w, letter="G", font_size_ratio=0.9)

from step5_create_masked import create_masked_stipple
 
# Create masked stippled image
masked_stipple = create_masked_stipple(
    stipple_pattern,
    block_letter,
    threshold=0.5  # Pixels below 0.5 are considered part of the mask
)

from create_meme import create_statistics_meme

# Create the final meme
create_statistics_meme(
    original_img=gray_image,
    stipple_img=stipple_pattern,
    block_letter_img=block_letter,
    masked_stipple_img=masked_stipple,
    output_path="my_statistics_meme.png",
    dpi=150,
    background_color="white"  # or "pink", "lightgray", etc.
)

# Display the final meme
from IPython.display import Image, display
display(Image("my_statistics_meme.png"))


```


### Explanation

This statistics meme provides a powerful visual demonstration of **selection bias** and its impact on statistical inference. The four-panel visualization walks through the progression from truth to biased estimate, making abstract statistical concepts tangible through image processing.

**Panel 1: Reality** â€” The original grayscale photograph represents the **true population**â€”the complete, unbiased ground truth we seek to understand. Every pixel contains information about the subject's features, lighting, and composition. In statistical terms, this is the parameter we wish to estimate: the full population distribution without any systematic exclusions.

**Panel 2: Your Sample** â€” The stippled (dot-pattern) version represents our **data collection process**â€”a sample of observations drawn from the population. The blue noise stippling algorithm strategically places black dots (data points) to preserve visual information while maintaining spatial distribution. This panel illustrates what we observe when we collect data: we don't see everything, but with proper sampling, the pattern of dots should still represent the underlying reality. The stippling preserves key featuresâ€”facial structure, hair, glasses, clothingâ€”demonstrating that even with sparse sampling, we can capture essential information about the population.

**Panel 3: Selection Bias** â€” The bold letter "G" represents the **systematic, non-random pattern of missing data**â€”the selection bias mechanism itself. This is not random missingness (MCAR), but rather **MNAR (Missing Not At Random)**, where the probability of missingness depends on unobserved or underlying data characteristics. The "G" shape is arbitrary but systematicâ€”it follows a specific pattern that is unrelated to the data's true structure. In real-world research, selection bias manifests when certain groups are systematically excluded: non-responders in surveys, patients lost to follow-up in clinical trials, or specific demographics underrepresented in studies.

**Panel 4: Estimate** â€” The final panel shows the **biased estimate**â€”what remains after selection bias has systematically removed data points. The white "G"-shaped void cuts through the stippled image, creating a systematic gap in our observations. Critical information is missing: facial features, clothing details, and other characteristics that would have been captured within the "G" region are now absent. This distorted view no longer accurately represents the true population. The estimate is **biased** because the missingness pattern (the "G") is systematic and non-random, creating systematic differences between what we observe and what actually exists in the population.

**Statistical Implications:** This visualization demonstrates a fundamental problem in statistics: when data is missing in a systematic pattern (MNAR), standard statistical methods that assume random missingness will produce **biased estimates**. The systematic exclusion creates a non-representative sample, leading to incorrect conclusions about the population. Just as the "G"-shaped gap distorts our visual understanding of the subject, selection bias distorts our statistical understanding of populations in real research scenarios.

**Real-World Relevance:** This meme mirrors countless real-world scenarios where selection bias undermines research validity:
- **Non-response bias**: When survey respondents differ systematically from non-respondents (e.g., healthier individuals more likely to participate in health surveys)
- **Attrition bias**: When participants who drop out of longitudinal studies differ from those who remain
- **Publication bias**: When studies with significant results are more likely to be published than null findings
- **Volunteer bias**: When study volunteers differ from the general population in ways related to the outcome of interest

The key takeaway is that **recognizing selection bias is the first step toward addressing it**. Through careful study design, appropriate statistical methods (e.g., inverse probability weighting, propensity score matching), sensitivity analyses, and transparent reporting of limitations, researchers can mitigate the impact of selection bias and draw more valid conclusions from their data.

